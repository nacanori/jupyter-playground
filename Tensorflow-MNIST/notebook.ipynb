{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# MNIST データセットを読み込む\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 回帰によるモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax(多項ロジスティック)関数は、多変量のロジスティック回帰を利用した多値分類のアルゴリズム。\n",
    "\n",
    "\n",
    "<img src=\"./img/softmax.png\" style=\"height: 40px\">\n",
    "\n",
    "\n",
    "Softmax 回帰では、x の重み合計を計算し、バイアスを加え、そして softmax を適用する。\n",
    "以下のようなイメージ。\n",
    "\n",
    "* y が目的変数 (0-9の数字)\n",
    "* x が説明変数 (256 階調のグレースケール、各ピクセルの濃淡を 0.0-1.0 で表したもの)\n",
    "* W が重み\n",
    "* b がバイアスの行列\n",
    "\n",
    "<img src=\"./img/softmax1.png\" style=\"height: 130px\">\n",
    "\n",
    "式で表すと、\n",
    "\n",
    "<img src=\"./img/softmax2.png\" style=\"height: 100px\">\n",
    "\n",
    "\n",
    "今回は、最適な重みとバイアスを推定するための誤差関数（損失関数）として、交差エントロピー (cross entropy) と呼ばれる指標を用います。この交差エントロピーが最も小さくなるように、TensorFlow に `GradientDescentOptimizer` として実装されている、勾配降下法 ( `Gradient descent` ) を用いて学習を進めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.119464, Accuracy: 0.194800\n",
      "Step: 100, Loss: 0.384369, Accuracy: 0.893200\n",
      "Step: 200, Loss: 0.337418, Accuracy: 0.906300\n",
      "Step: 300, Loss: 0.315937, Accuracy: 0.912500\n",
      "Step: 400, Loss: 0.313405, Accuracy: 0.911900\n",
      "Step: 500, Loss: 0.308517, Accuracy: 0.915000\n",
      "Step: 600, Loss: 0.299391, Accuracy: 0.912800\n",
      "Step: 700, Loss: 0.297278, Accuracy: 0.913800\n",
      "Step: 800, Loss: 0.294411, Accuracy: 0.916700\n",
      "Step: 900, Loss: 0.288827, Accuracy: 0.917600\n"
     ]
    }
   ],
   "source": [
    "# 入力データ格納用の 784px 分のプレースホルダを作成\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# 重み (784 ✕ 10 の行列) の Variable を定義\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# バイアス (長さ 10 の行列) の Variable を定義\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# ソフトマックス回帰による予測式を定義\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 出力データ (予測値) 格納用のプレースホルダ\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 交差エントロピーが最も小さくなるように、学習を行う式を定義\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# 実際の値と予測された値が同じであるか確認\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "# 平均値を求め、予測精度を求める\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 初期化\n",
    "tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "# 学習を 1000 回繰り返す\n",
    "for i in range(1000):\n",
    "    # 訓練用データから 100 件取得\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # train_spep を実行\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "        print ('Step: %d, Loss: %f, Accuracy: %f' % (i, loss_val, acc_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.29550225, \t Train Accuracy: 0.91685456, \t Test Accuracy: 0.9177\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "train_loss = sess.run(loss, feed_dict={x: mnist.train.images, y_: mnist.train.labels})\n",
    "train_accuracy = sess.run(accuracy, feed_dict={x: mnist.train.images, y_: mnist.train.labels})\n",
    "test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "\n",
    "# 学訓練エラー、訓練精度、テスト精度\n",
    "std_output = 'Train Loss: %s, \\t Train Accuracy: %s, \\t Test Accuracy: %s'\n",
    "print(std_output % (train_loss, train_accuracy, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "print(\"Model saved in path: %s\" % save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
